


import pandas as pd
import numpy as np
import re
import requests
from collections import Counter





df = pd.read_csv('../data/raw/PhiUSIIL_Phishing_URL_Dataset.csv')
df.head()





df.describe()


df.dtypes


df.isnull().sum()


df.columns


df.shape








df.drop(columns=['FILENAME'], inplace=True)





def contains_foreign_characters(url):
    '''A function to find out urls with non-ASCII characters'''
    return bool(re.search(r'[^\x00-\x7F]', url))


df['url_contains_foreign'] = df['URL'].apply(contains_foreign_characters)


df[df['url_contains_foreign'] == True]





df.drop(index=[127839, 158380], inplace=True)





df['domain_contains_foreign'] = df['Domain'].apply(contains_foreign_characters)


df[df['domain_contains_foreign'] == True]





df['tld_contains_foreign'] = df['TLD'].apply(contains_foreign_characters)


df[df['tld_contains_foreign'] == True]





df['title_contains_foreign'] = df['Title'].apply(contains_foreign_characters)


df[df['title_contains_foreign'] == True]


title_foreign = df[df['title_contains_foreign'] == True]


title_foreign[['Title']]





df.drop(df[df['title_contains_foreign'] == True].index, inplace=True)


df[df['title_contains_foreign'] == True]





col_index = df.columns.get_loc('url_contains_foreign')
col_to_delete = df.columns[col_index:]
df.drop(columns=col_to_delete)


df.drop(columns=col_to_delete, inplace=True)


df.head()





df.duplicated().sum()


df.duplicated('URL').sum()


df.duplicated('Domain').sum()


df.duplicated('Title').sum()


df[['URL', 'Title']]





df_url_dup = df[df.duplicated('URL', keep=False)]


# Check which features differ among duplicated URLs
diff_features = df_url_dup.groupby('URL').agg(lambda x: x.nunique() > 1)

# Identify columns with differences
columns_with_diff = diff_features.columns[diff_features.any()]
print("Columns with differences among duplicated URLs:", columns_with_diff)

# Show the differing values for these columns
for column in columns_with_diff:
    print(f"Differences in column '{column}':")
    print(df_url_dup.groupby('URL')[column].apply(list))





df.to_csv('../data/interim/partial_cleaned.csv')



